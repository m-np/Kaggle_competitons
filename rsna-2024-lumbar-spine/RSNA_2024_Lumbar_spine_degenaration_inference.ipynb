{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":187278429,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This code is following the inference notebook shared [here](https://www.kaggle.com/code/samu2505/rsna-lumbar-inference-lb-0-84-cv-0-54) \n\nTraining code link in the following [NB](https://www.kaggle.com/code/zoro666/rsna-2024-lumbar-spine-1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os, gc, sys, copy, pickle\nfrom pathlib import Path\nimport glob\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport math\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom joblib import Parallel, delayed\nimport multiprocessing as mp\n\nimport albumentations as A\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.cuda.amp as amp\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nfrom torch.utils.data import WeightedRandomSampler\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport timm\n\nimport cv2\ncv2.setNumThreads(0)\nimport PIL\nimport pydicom\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:47:47.994917Z","iopub.execute_input":"2024-07-07T19:47:47.995287Z","iopub.status.idle":"2024-07-07T19:47:57.931813Z","shell.execute_reply.started":"2024-07-07T19:47:47.995259Z","shell.execute_reply":"2024-07-07T19:47:57.930733Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def seeding(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    torch.manual_seed(SEED)\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(SEED)\n        torch.cuda.manual_seed_all(SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n#     os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n#     tf.random.set_seed(SEED)\n#     keras.utils.set_random_seed(seed=SEED)\n    print('seeding done!!!')\n\ndef flush():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:48:46.456019Z","iopub.execute_input":"2024-07-07T19:48:46.456418Z","iopub.status.idle":"2024-07-07T19:48:46.463090Z","shell.execute_reply.started":"2024-07-07T19:48:46.456387Z","shell.execute_reply":"2024-07-07T19:48:46.462073Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"CONFIG = dict(\n    project_name = \"RSNA-2024-Lumbar-Spine-Classification-Torch-RZoro\",\n    artifact_name = \"rsnaEffNetModel\",\n    load_kernel = None,\n    load_last = True,\n    n_folds = 5,\n    backbone = \"efficientnet_b0.ra_in1k\", # tf_efficientnetv2_s_in21ft1k\n    img_size = 384,\n    n_slice_per_c = 16,\n    in_chans = 3,\n\n    drop_rate = 0.,\n    drop_rate_last = 0.3,\n    drop_path_rate = 0.,\n    p_mixup = 0.5,\n    p_rand_order_v1 = 0.2,\n    lr = 4e-4, # 1e-3, 8e-4, 5e-4, 4e-4\n\n    out_dim = 3,\n    epochs = 50,\n    batch_size = 16,\n#     patience = 7,\n    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\",\n    seed = 2024,\n    log_wandb = True,\n    with_clip = True,\n)\n\nCONFIG['patience'] = math.ceil(0.2 * CONFIG['epochs'])\n\nseeding(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:49:55.142548Z","iopub.execute_input":"2024-07-07T19:49:55.142953Z","iopub.status.idle":"2024-07-07T19:49:55.153464Z","shell.execute_reply.started":"2024-07-07T19:49:55.142922Z","shell.execute_reply":"2024-07-07T19:49:55.152258Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"seeding done!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"DATA_PATH = Path(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification\")\nos.listdir(DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:50:04.154491Z","iopub.execute_input":"2024-07-07T19:50:04.154850Z","iopub.status.idle":"2024-07-07T19:50:04.163802Z","shell.execute_reply.started":"2024-07-07T19:50:04.154824Z","shell.execute_reply":"2024-07-07T19:50:04.162562Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['sample_submission.csv',\n 'train_images',\n 'train_series_descriptions.csv',\n 'train.csv',\n 'train_label_coordinates.csv',\n 'test_series_descriptions.csv',\n 'test_images']"},"metadata":{}}]},{"cell_type":"code","source":"sample_df = pd.read_csv(DATA_PATH/\"sample_submission.csv\")\ntest_desc = pd.read_csv(DATA_PATH/\"test_series_descriptions.csv\")\ntrain_desc = pd.read_csv(DATA_PATH/\"train_series_descriptions.csv\")\ntrain_main = pd.read_csv(DATA_PATH/\"train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:50:10.454923Z","iopub.execute_input":"2024-07-07T19:50:10.455702Z","iopub.status.idle":"2024-07-07T19:50:10.508394Z","shell.execute_reply.started":"2024-07-07T19:50:10.455665Z","shell.execute_reply":"2024-07-07T19:50:10.507256Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Create dataset","metadata":{}},{"cell_type":"code","source":"# define the base path for test images\nbase_path = f\"{str(DATA_PATH)}/test_images\"\n\n# function to get image paths for a series\ndef get_image_paths(row):\n    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_id']))\n    if os.path.exists(series_path):\n        return [\n            os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))\n        ]\n    return []\n\n# Mapping of series_description to conditions\ncondition_mapping = {\n    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n}\n\n# Create a list to store the expanded rows\nexpanded_rows = []\n\n# Expand the dataframe by adding new rows for each file path\nfor index, row in test_desc.iterrows():\n    image_paths = get_image_paths(row)\n    conditions = condition_mapping.get(row['series_description'], {})\n    if isinstance(conditions, str):  # Single condition\n        conditions = {'left': conditions, 'right': conditions}\n    for side, condition in conditions.items():\n        for image_path in image_paths:\n            expanded_rows.append({\n                'study_id': row['study_id'],\n                'series_id': row['series_id'],\n                'series_description': row['series_description'],\n                'image_path': image_path,\n                'condition': condition,\n                'row_id': f\"{row['study_id']}_{condition}\"\n            })\n\n# Create a new dataframe from the expanded rows\nexpanded_test_desc = pd.DataFrame(expanded_rows)\n\ntest_data = expanded_test_desc.copy()\ntest_data['target'] = 0\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:51:04.985152Z","iopub.execute_input":"2024-07-07T19:51:04.985544Z","iopub.status.idle":"2024-07-07T19:51:05.038471Z","shell.execute_reply.started":"2024-07-07T19:51:04.985513Z","shell.execute_reply":"2024-07-07T19:51:05.037517Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   study_id   series_id series_description  \\\n0  44036939  2828203845        Sagittal T1   \n1  44036939  2828203845        Sagittal T1   \n2  44036939  2828203845        Sagittal T1   \n3  44036939  2828203845        Sagittal T1   \n4  44036939  2828203845        Sagittal T1   \n\n                                          image_path  \\\n0  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n1  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n2  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n3  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n4  /kaggle/input/rsna-2024-lumbar-spine-degenerat...   \n\n                         condition                                    row_id  \\\n0  left_neural_foraminal_narrowing  44036939_left_neural_foraminal_narrowing   \n1  left_neural_foraminal_narrowing  44036939_left_neural_foraminal_narrowing   \n2  left_neural_foraminal_narrowing  44036939_left_neural_foraminal_narrowing   \n3  left_neural_foraminal_narrowing  44036939_left_neural_foraminal_narrowing   \n4  left_neural_foraminal_narrowing  44036939_left_neural_foraminal_narrowing   \n\n   target  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>series_id</th>\n      <th>series_description</th>\n      <th>image_path</th>\n      <th>condition</th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44036939</td>\n      <td>2828203845</td>\n      <td>Sagittal T1</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>left_neural_foraminal_narrowing</td>\n      <td>44036939_left_neural_foraminal_narrowing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44036939</td>\n      <td>2828203845</td>\n      <td>Sagittal T1</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>left_neural_foraminal_narrowing</td>\n      <td>44036939_left_neural_foraminal_narrowing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>44036939</td>\n      <td>2828203845</td>\n      <td>Sagittal T1</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>left_neural_foraminal_narrowing</td>\n      <td>44036939_left_neural_foraminal_narrowing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44036939</td>\n      <td>2828203845</td>\n      <td>Sagittal T1</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>left_neural_foraminal_narrowing</td>\n      <td>44036939_left_neural_foraminal_narrowing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>44036939</td>\n      <td>2828203845</td>\n      <td>Sagittal T1</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>left_neural_foraminal_narrowing</td>\n      <td>44036939_left_neural_foraminal_narrowing</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"label2id = {\"Normal/Mild\": 0, \"Moderate\": 1, \"Severe\": 2}\nid2label = {v:k for k,v in label2id.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:51:21.510179Z","iopub.execute_input":"2024-07-07T19:51:21.510718Z","iopub.status.idle":"2024-07-07T19:51:21.515888Z","shell.execute_reply.started":"2024-07-07T19:51:21.510686Z","shell.execute_reply":"2024-07-07T19:51:21.514686Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:51:27.348578Z","iopub.execute_input":"2024-07-07T19:51:27.348953Z","iopub.status.idle":"2024-07-07T19:51:27.354759Z","shell.execute_reply.started":"2024-07-07T19:51:27.348922Z","shell.execute_reply":"2024-07-07T19:51:27.353600Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, dataframe, transform=None, label_name='target'):\n        self.dataframe = dataframe\n        self.transform = transform\n        self.label = dataframe.loc[:, label_name]\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        image_path = self.dataframe['image_path'][index]\n        image = load_dicom(image_path)  # Define this function to load your DICOM images\n        target = self.dataframe['target'][index]\n        \n        if self.transform:\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n            image = self.transform(image=image)['image']\n            image = image.transpose(2, 0, 1).astype(np.float32) / 255.\n\n        return image, torch.tensor(target).float()\n    \n    def get_labels(self):\n        return self.label","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:52:04.127465Z","iopub.execute_input":"2024-07-07T19:52:04.127866Z","iopub.status.idle":"2024-07-07T19:52:04.136598Z","shell.execute_reply.started":"2024-07-07T19:52:04.127833Z","shell.execute_reply":"2024-07-07T19:52:04.135333Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_transforms(height, width):\n    train_tsfm = A.Compose([\n        A.Resize(height=height, width=width, interpolation=cv2.INTER_CUBIC, p=1.0), # also INTER_LANCZOS4\n        # Geometric augmentations\n        A.Perspective(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=(-30, 30), p=0.5), \n        \n        A.CenterCrop(height=height, width=width, p=1.0),\n    ])\n    \n    valid_tsfm = A.Compose([\n        A.Resize(height=height, width=width),\n    ])\n    return {\"train\": train_tsfm, \"eval\": valid_tsfm}","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:52:24.967582Z","iopub.execute_input":"2024-07-07T19:52:24.968267Z","iopub.status.idle":"2024-07-07T19:52:24.974592Z","shell.execute_reply.started":"2024-07-07T19:52:24.968226Z","shell.execute_reply":"2024-07-07T19:52:24.973570Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# def get_dataloaders(data, cfg, split=\"train\"):\n#     img_size = cfg['img_size']\n#     height, width = img_size, img_size\n#     tsfm = get_transforms(height=height, width=width)\n#     if split == 'train':\n#         tr_tsfm = tsfm['train']\n#         ds = CustomDataset(data, transform=tr_tsfm)\n#         labels = ds.get_labels()\n# #         class_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels))\n#         class_weights = torch.tensor([1, 2, 4])\n#         samples_weights = class_weights[labels]\n# #         print(class_weights)\n#         sampler = WeightedRandomSampler(weights=samples_weights, \n#                                         num_samples=len(samples_weights), \n#                                         replacement=True)\n\n#         dls = DataLoader(ds, \n#                          batch_size=cfg['batch_size'], \n#                          sampler=sampler, \n#                          num_workers=os.cpu_count(), \n#                          pin_memory=True, \n#                          drop_last=True)\n        \n#     elif split == 'valid' or split == 'test':\n#         eval_tsfm = tsfm['eval']\n#         ds = CustomDataset(data, transform=eval_tsfm)\n#         dls = DataLoader(ds, \n#                          batch_size=2*cfg['batch_size'], \n#                          sampler=SequentialSampler(ds),\n#                          num_workers=os.cpu_count(), \n#                          pin_memory=True,\n#                          drop_last=False)\n#     else:\n#         raise Exception(\"Split should be 'train' or 'valid' or 'test'!!!\")\n#     return dls\n\ndef get_dataloaders(data, cfg, split=\"train\"):\n    img_size = cfg['img_size']\n    height, width = img_size, img_size\n    tsfm = get_transforms(height=height, width=width)\n    if split == 'train':\n        tr_tsfm = tsfm['train']\n        ds = CustomDataset(data, transform=tr_tsfm)\n        labels = ds.get_labels()\n#         class_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels))\n        class_weights = torch.tensor([1, 2, 4])\n        samples_weights = class_weights[labels]\n#         print(class_weights)\n        sampler = WeightedRandomSampler(weights=samples_weights, \n                                        num_samples=len(samples_weights), \n                                        replacement=True)\n\n        dls = DataLoader(ds, \n                         batch_size=cfg['batch_size'], \n                         sampler=sampler, \n                         num_workers=os.cpu_count(), \n                         drop_last=True, \n                         pin_memory=True)\n        \n    elif split == 'valid' or split == 'test':\n        eval_tsfm = tsfm['eval']\n        ds = CustomDataset(data, transform=eval_tsfm)\n        dls = DataLoader(ds, \n                         batch_size=2*cfg['batch_size'], \n                         shuffle=False, \n                         num_workers=os.cpu_count(), \n                         drop_last=False, \n                         pin_memory=True)\n    else:\n        raise Exception(\"Split should be 'train' or 'valid' or 'test'!!!\")\n    return dls","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:31:57.075093Z","iopub.execute_input":"2024-07-07T20:31:57.075590Z","iopub.status.idle":"2024-07-07T20:31:57.086524Z","shell.execute_reply.started":"2024-07-07T20:31:57.075559Z","shell.execute_reply":"2024-07-07T20:31:57.085257Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"class TimmModel(nn.Module):\n    def __init__(self, backbone, pretrained=False):\n        super(TimmModel, self).__init__()\n\n        self.encoder = timm.create_model(\n            backbone,\n            num_classes=CONFIG[\"out_dim\"],\n            features_only=False,\n            drop_rate=CONFIG[\"drop_rate\"],\n            drop_path_rate=CONFIG[\"drop_path_rate\"],\n            pretrained=pretrained\n        )\n\n        if 'efficient' in backbone:\n            hdim = self.encoder.conv_head.out_channels\n            self.encoder.classifier = nn.Identity()\n        elif 'convnext' in backbone:\n            hdim = self.encoder.head.fc.in_features\n            self.encoder.head.fc = nn.Identity()\n\n\n        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=CONFIG[\"drop_rate\"], bidirectional=True, batch_first=True)\n        self.head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(CONFIG[\"drop_rate_last\"]),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, CONFIG[\"out_dim\"]),\n        )\n\n    def forward(self, x):\n        feat = self.encoder(x)\n        feat, _ = self.lstm(feat)\n        feat = self.head(feat)\n        return feat","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:53:07.828537Z","iopub.execute_input":"2024-07-07T19:53:07.829236Z","iopub.status.idle":"2024-07-07T19:53:07.838086Z","shell.execute_reply.started":"2024-07-07T19:53:07.829198Z","shell.execute_reply":"2024-07-07T19:53:07.836862Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Predictions\nHere we have code for prediction with and without test time augmentation\n\nTest time augmentation (TTA) is useful for test images that the model is pretty unsure. It takes approximately 2 hours as compared to the normal prediction which takes roughly 30 minutes","metadata":{}},{"cell_type":"code","source":"FLIPS = [None, [-1], [-2], [-2, -1]]\n\ndef inference_loop(model, loader):\n    model.to(CONFIG[\"device\"])\n    model.eval()\n    preds = np.empty((0, 3))\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            images, labels = batch\n            images = images.to(CONFIG[\"device\"], non_blocking=True)\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n#                 logits = model(images.to(torch.float32))\n                logits = model(images)\n#                 logits = logits.mean(axis=1).softmax(dim=-1)\n                logits = logits.softmax(dim=-1)\n                preds = np.concatenate([preds, logits.detach().cpu().numpy()])\n    np.save('preds.npy', preds)\n    \n    \ndef tta_inference_loop(model, loader):\n    model.to(CONFIG[\"device\"])\n    model.eval()\n    preds = np.empty((0, 3))\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            images, labels = batch\n            images = images.to(CONFIG[\"device\"], non_blocking=True)\n            pred_tta = []\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                for f in FLIPS:\n                    logits = model(torch.flip(images, f) if f is not None else images)\n                    logits = logits.softmax(dim=-1)\n                    pred_tta.append(logits.detach().cpu().numpy())\n#                 preds = np.concatenate([preds, logits.detach().cpu().numpy()])\n                preds = np.concatenate([preds, np.mean(pred_tta, 0)])\n    np.save('preds.npy', preds)\n#     return preds","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:11:53.046048Z","iopub.execute_input":"2024-07-07T20:11:53.046860Z","iopub.status.idle":"2024-07-07T20:11:53.057951Z","shell.execute_reply.started":"2024-07-07T20:11:53.046826Z","shell.execute_reply":"2024-07-07T20:11:53.056876Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Load model weights\nThe model is trained for 1 fold using pytorch lightning rsna-pytorchlightning and the model weights are stored using Weights and Biases artifacts and you can find them [here](https://www.kaggle.com/code/zoro666/rsna-2024-load-weights/notebook)","metadata":{}},{"cell_type":"code","source":"weights_path = \"/kaggle/input/rsna-2024-load-weights/model_weights.pth\"\nweights = torch.load(weights_path, map_location=torch.device(\"cpu\"))\nmodel = TimmModel(backbone=CONFIG[\"backbone\"], pretrained=False)\nmodel.load_state_dict(weights)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:30:49.006018Z","iopub.execute_input":"2024-07-07T20:30:49.006413Z","iopub.status.idle":"2024-07-07T20:30:49.710711Z","shell.execute_reply.started":"2024-07-07T20:30:49.006384Z","shell.execute_reply":"2024-07-07T20:30:49.709657Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"dls = get_dataloaders(test_data, CONFIG, split=\"test\")\n# inference_loop(model, dls)\ntta_inference_loop(model, dls)\n# _ = Parallel(n_jobs=mp.cpu_count())(\n#     delayed(inference_loop(model, dls))\n# )\n\npreds = np.load('preds.npy')","metadata":{"execution":{"iopub.status.busy":"2024-07-07T20:31:59.936227Z","iopub.execute_input":"2024-07-07T20:31:59.936617Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b3578478244a6ab62a29af5af49764"}},"metadata":{}}]},{"cell_type":"code","source":"levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\n\n# Function to update row_id with levels\ndef update_row_id(row, levels):\n    level = levels[row.name % len(levels)]\n    return f\"{row['study_id']}_{row['condition']}_{level}\"\n\n# Update row_id in expanded_test_desc to include levels\nexpanded_test_desc['row_id'] = expanded_test_desc.apply(lambda row: update_row_id(row, levels), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expanded_test_desc[[\"normal_mild\",\"moderate\",\"severe\"]] = preds\n\nfinal_df = expanded_test_desc[[\"row_id\",\"normal_mild\",\"moderate\",\"severe\"]]\n\ntarget_cols = sample_df.columns.tolist()\nfinal_df = final_df.groupby('row_id').sum().reset_index()\n# normalize the columns\nfinal_df[target_cols[1:]] = final_df[target_cols[1:]].div(final_df[target_cols[1:]].sum(axis=1), axis=0)\nfinal_df[target_cols].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}